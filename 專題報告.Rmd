---
title: "利用R語言建構預測模型-以心臟病發作之狀態為例"
author: "吳忠憲、曾增凱、曾苡嘉"
date: "2024-08-12"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE,message = FALSE)
library(reticulate)
# use_python("C:\\Users\\USER\\anaconda3")
```

#### 安裝套件
```{r }
library(showtext)
library(caret)
library(tfruns)
library(showtext)
library(ggplot2)
library(Boruta)
library(party)
library(randomForest)
library(class)
library(highcharter)
library(e1071)
library(SciViews)
library(png)
library(magick)
library(tidyverse)
library(neuralnet)
library(keras)
library(tensorflow)
showtext_auto(enable = TRUE)
```

https://reurl.cc/0dxArA

#### 載入資料集及資料理解
|名稱|描述|類型|意義|
|----|----|----|----|
|Age|年齡|數值型|年齡是心臟病的風險因素之一。隨著年齡增長，心臟病的風險也會增加。|
|Sex|性別|類別型（通常為 0 和 1，其中 0 代表女性，1 代表男性）|性別也會影響心臟病的風險。例如，男性在某些年齡段的心臟病風險通常高於女性。|
|cp|胸痛類型|類別型（通常有 4 個類別，如 1: 胸痛，2: 壓迫感，3: 針刺感，4: 不適感）|胸痛是心臟病的一個重要症狀，不同類型的胸痛可能指示不同類型的心臟病。|
|trtbps|靜息血壓|數值型（以 mmHg 為單位）|靜息血壓升高可能是心臟病的風險因素之一。|
|chol|血清膽固醇|數值型（以 mg/dl 為單位）|高膽固醇水平與心臟病風險增加相關。|
|fbs|空腹血糖|類別型（0 或 1，其中 1 表示空腹血糖 > 120 mg/dl）|高空腹血糖水平可能與糖尿病相關，糖尿病是一個心臟病風險因素。|
|restecg|靜態心電圖結果|類別型（通常有 3 個類別，如 0: 正常，1: ST-T 波異常，2: 左心室肥厚）|心電圖的異常結果可以幫助診斷心臟病。|
|thalachh|最大心率|數值型|最大心率在運動時的測量值，低心率可能與心臟病風險增加相關。|
|exng|運動誘發的心絞痛|類別型（0 或 1，其中 1 表示有心絞痛）|運動誘發的心絞痛是一個心臟病的症狀。|
|oldpeak|運動引起的 ST 段抑制|數值型（通常是負數，表示 ST 段的下凹）|ST 段抑制可能是心臟病的指示。|
|slp|峰值運動 ST 段的斜率|類別型（通常有 3 個類別，如 1: 上升，2: 平坦，3: 下凹）|ST 段斜率可以幫助評估心臟病的嚴重程度。|
|caa|冠狀動脈擴張的數量（通過造影顯示）|數值型（0 到 3 之間的數字）|較多的血管問題可能表示心臟病的風險更高。|
|thall|地中海貧血（影響血液中的紅血球）|類別型（通常有 3 個類別，如 1: 正常，2: 固定缺失，3: 可逆缺失）|地中海貧血的類型可以影響心臟病的診斷。|
|output|心臟病的存在與否|類別型（0 或 1，其中 1 表示存在心臟病）|目標變量，用來表示患者是否被診斷為心臟病。|


```{r }
getwd()
heart = read.csv('heart.csv')
dim(heart)
head(heart)
```

```{r }
sapply(heart, function(x) sum(is.na(x)))
```


#### 數值資料及類別資料區分
```{r }
heart_num<-heart[,c("age","trtbps","chol","thalachh","oldpeak","caa")]
heart_cat <- heart[,-c(1,4,5,8,10,12)]
heart_cat <- as.data.frame(heart_cat)

```

#### 盒鬚圖
```{R}
library(shiny)
library(ggplot2)

# Define the dataset (replace with your actual data)
heart_num_plt <- data.frame(
  age = rnorm(100, 50, 10),
  trtbps = rnorm(100, 120, 15),
  chol = rnorm(100, 200, 30),
  thalachh = rnorm(100, 150, 20),
  oldpeak = rnorm(100, 1, 0.5),
  caa = sample(0:4, 100, replace = TRUE)
)

# Define UI for application
ui <- fluidPage(
    titlePanel("Interactive Boxplot"),
    sidebarLayout(
        sidebarPanel(
            selectInput("variable", 
                        "Choose a variable:", 
                        choices = c("年齡" = "age",
                                    "血壓" = "trtbps",
                                    "膽固醇" = "chol",
                                    "最大心率" = "thalachh",
                                    "運動後的心電圖指數" = "oldpeak",
                                    "冠狀動脈擴張數量" = "caa"))
        ),
        mainPanel(
            plotOutput("boxplot")
        )
    )
)

# Define server logic
server <- function(input, output) {
    output$boxplot <- renderPlot({
        # Get the selected variable
        selected_var <- input$variable
        
        # Create boxplot
        tmp <- boxplot(heart_num[[selected_var]], 
                       main=input$variable, 
                       horizontal=F, 
                       col="#CF9E9E")
        
        # Add text to the plot
        text(y=tmp$stats, 
             labels=round(tmp$stats, 2), 
             col = "#007979", 
             x=1.3, 
             cex=1.0, 
             font=2)
    })
}

# Run the application 
shinyApp(ui = ui, server = server)

```


#### 類別變數之長條圖
```{R}

# Define the dataset (replace with your actual data)
heart_cat_bar <- data.frame(
  sex = sample(c("Female", "male"), 100, replace = TRUE),
  cp = sample(c("Typical Angina", "Atypical Angina", "Non-Anginal Pain", "Asymptomatic"), 100, replace = TRUE),
  fbs = sample(c("False", "True"), 100, replace = TRUE),
  restecg = sample(c("Normal", "ST-T Wave Abnormality", "Left Ventricular Hypertrophy"), 100, replace = TRUE),
  exng = sample(c("No", "Yes"), 100, replace = TRUE),
  slp = sample(c("Up", "Flat", "Down"), 100, replace = TRUE),
  thall = sample(c("Normal", "Fixed Defect", "Reversable Defect"), 100, replace = TRUE)
)

# Define UI for application
ui <- fluidPage(
    titlePanel("Interactive Barplot"),
    sidebarLayout(
        sidebarPanel(
            selectInput("variable", 
                        "Choose a variable:", 
                        choices = c("性別" = "sex",
                                    "胸痛類型" = "cp",
                                    "空腹血糖狀況" = "fbs",
                                    "靜態心電圖結果" = "restecg",
                                    "運動後心絞痛情況" = "exng",
                                    "心電圖ST段斜率的類別" = "slp",
                                    "地中海貧血類型" = "thall"))
        ),
        mainPanel(
            plotOutput("barplot")
        )
    )
)

# Define server logic
server <- function(input, output) {
    output$barplot <- renderPlot({
        # Get the selected variable
        selected_var <- input$variable
        
        # Create barplot
        barplot(table(heart_cat[[selected_var]]), 
                main=input$variable, 
                col = "#C4E1FF",
                las = 1) # `las=2` makes the axis labels perpendicular to the axis
    })
}

# Run the application 
shinyApp(ui = ui, server = server)


```


#### 數值變數之散佈圖
```{R }
# diag.panel=panel.hist
# upper.panel=panel.cor
# plot(heart_num,main='數值變數之散佈圖')

pairs(heart_num, 
      lower.panel = panel.cor,  # 下三角顯示相關係數
      diag.panel = panel.hist)  # 對角線顯示長條圖
```

#### 相關分析圖
```{r}
corrplot::corrplot(cor(heart[-8]))

```


#### 正規化
```{r}
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

# 運用隱式迴圈函數逐欄(含y)正規化後再將等長串列轉為資料框
heart_norm <- as.data.frame(lapply(heart_num, normalize))

heart_all <- cbind(heart_cat, heart_norm)
heart_all$output <- as.factor(heart_all$output)


```


#### 創建訓練集及測試集

```{r}

set.seed(456)

inTrain <- createDataPartition(heart_all[,13],
                               p = 0.7,
                               list = FALSE)
heart_train <- heart_all[inTrain,]
heart_test <- heart_all[-inTrain,]
```

#### 分割後資料大小
```{r}
cat('訓練集：',dim(heart_train))
cat('\n測試集：',dim(heart_test))


```

#### 確認訓練集在患有心臟病的人數是否平衡
```{r}
a = table(heart_train$output)
b = barplot(a,col=c("lightcoral","#BBFFBB"),main='Class Distribution',ylim = c(0,130),ylab = '樣本數量',xlab = '是否患有心臟病',names.arg = c('無','有'),cex.names = 2,cex.axis = 2,cex.lab=2,cex.main=2)
text(x=b,y=a+6,labels = a,cex = 2)
```

#### 確認測試集在患有心臟病的人數是否平衡

```{r}
a = table(heart_test$output)
b = barplot(a,col=c("lightcoral","#BBFFBB"),main='Class Distribution',ylim = c(0,60),ylab = '樣本數量',xlab = '是否患有心臟病',names.arg = c('無','有'),cex.names = 2,cex.axis = 2,cex.lab=2,cex.main=2)
text(x=b,y=a+6,labels = a,cex = 2)
```


#### 迴歸模型

```{R}
set.seed(456)
train_control <- trainControl(method = "cv", number = 10)

model_glm <- caret::train(output ~ ., 
               data = heart_train, 
               method = "glm", 
               family = binomial,
               trControl = train_control)

print(model_glm)
summary(model_glm)

predictTrain = predict(model_glm, data = heart_train, type = "prob")

# print(predictTrain)
table(heart_train$output, ifelse(predictTrain[1]>predictTrain[2], 0, 1))

predictTest = predict(model_glm, newdata = heart_test, type = "prob")

pre_lr <- ifelse(predictTest[1]>predictTest[2], 0, 1)
pre_lm <- as.factor(pre_lr)
cm_lr <- confusionMatrix(pre_lm,heart_test$output)
cm_lr
```




#### 決策樹
```{R}
library(party)
library(caret)


set.seed(456)

# 制定參數
train_control <- trainControl(
  method = "cv",          # 使用交叉驗證
  number = 10             # 10 折交叉驗證
)

# 定義參數
tune_grid <- expand.grid(
  maxdepth = c(1, 2, 3, 4, 5)  
)

train_model <- function(maxdepth) {
  model <- ctree(
    output ~ .,                
    data = heart_train,        
    controls = ctree_control(maxdepth = maxdepth)  # 設置 maxdepth
  )
  # 使用交叉驗證計算模型性能
  pred <- predict(model, heart_train[, -8])
  cm <- confusionMatrix(pred, heart_train$output)
  return(cm$overall["Accuracy"])
}

# 進行網路搜尋
results <- data.frame(
  maxdepth = integer(),
  Accuracy = numeric()
)

for (depth in tune_grid$maxdepth) {
  accuracy <- train_model(depth)
  results <- rbind(results, data.frame(
    maxdepth = depth,
    Accuracy = accuracy
  ))
}

# 找到最佳參數组合
best_params <- results[which.max(results$Accuracy), ]
print(best_params)

# 使用最佳參數訓練模型
best_model <- ctree(
  output ~ .,                
  data = heart_train,        
  controls = ctree_control(maxdepth = best_params$maxdepth)  # 使用最佳的maxdepth
)

# 使用測試集進行預測
pre_df <- predict(best_model, heart_test[, -8])

# 計算測試集的混淆矩陣
cm_ct <- confusionMatrix(pre_df, heart_test$output)
print(cm_ct)

# 繪製決策樹
plot(best_model, type = "simple")

```

#### 隨機森林

```{R}
set.seed(350)
library(randomForest)
library(caret)

# 定義參數
mtry_values <- c(1, 2, 3, 4, 5)
ntree_values <- c(100, 300, 500)
nodesize_values <- c(1, 5, 10)

# 創建空的數據框
results <- data.frame(
  mtry = integer(),
  ntree = integer(),
  nodesize = integer(),
  Accuracy = numeric()
)


for (mtry in mtry_values) {
  for (ntree in ntree_values) {
    for (nodesize in nodesize_values) {
      model <- randomForest(
        output ~ .,               
        data = heart_train,        
        ntree = ntree,             
        mtry = mtry,               
        nodesize = nodesize,       
        proximity = TRUE,        
        importance = TRUE          
      )
      
      pre_rf <- predict(model, heart_test[,-8])
      cm_rf <- confusionMatrix(pre_rf, heart_test$output)
      
      results <- rbind(results, data.frame(
        mtry = mtry,
        ntree = ntree,
        nodesize = nodesize,
        Accuracy = cm_rf$overall['Accuracy']
      ))
    }
  }
}

# 找到最佳参数组合
best_params <- results[which.max(results$Accuracy), ]
print(best_params)


learn_rf <- randomForest(output~.,data = heart_train,ntree=best_params$ntree,
             mtry=best_params$mtry,
             nodesize = best_params$nodesize
)
pre_rf <-predict(learn_rf,heart_test[,-8])
cm_rf <- confusionMatrix(pre_rf,heart_test$output)
cm_rf
```



### KNN

```{R}
set.seed(456)
train_control <- trainControl(
  method = "cv",          # 使用交叉驗證
  number = 10             # 10 折交叉驗證
)

knn_Fit <- caret::train(
  output~., 
  data = heart_train,
  method = "knn",
  trControl = train_control, 
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(k = 1:10),
  metric     = "Accuracy"
)

knn_Fit

#plot(knn_Fit)

knn_Fit.predict <- predict(knn_Fit, newdata = heart_test, type = "raw")
table(heart_test[, 8], pred=knn_Fit.predict)

cm_knn <- confusionMatrix(knn_Fit.predict,heart_test$output)
cm_knn




```





#### 支援向量機
```{R}
set.seed(456)


# 確保因子水平有效
if (!is.factor(heart_train$output)) {
  heart_train$output <- as.factor(heart_train$output)
}
if (!is.factor(heart_test$output)) {
  heart_test$output <- as.factor(heart_test$output)
}

levels(heart_train$output) <- make.names(levels(heart_train$output))
levels(heart_test$output) <- make.names(levels(heart_test$output))

# 定義 SVM 參數網格
tune_grid <- expand.grid(
  .sigma = c(0.01, 0.03, 0.1, 0.3), # gamma 在 e1071 中稱為 sigma
  .C = c(1, 10, 100, 1000) # cost 參數
)

# 訓練 SVM 模型
train_control <- trainControl(
  method = "cv",  # 交叉驗證
  number = 10,    # 10 折交叉驗證
  summaryFunction = twoClassSummary, # 二類問題的評價指標
  classProbs = TRUE  # 計算類別的機率
)

svm_model <- caret::train(
  output ~ ., 
  data = heart_train,
  method = "svmRadialSigma",  # 使用svmRadialSigma
  tuneGrid = tune_grid,
  trControl = train_control,
  metric = "Accuracy"  # 使用準確率作為評價標準
)

# 顯示最佳的參數组合
print(svm_model$bestTune)

# 使用最佳的 gamma 和 cost 參數
best_gamma <- svm_model$bestTune$sigma
best_cost <- svm_model$bestTune$C

# 訓練最佳的模型
learn_svm_best <- svm(
  output ~ ., 
  data = heart_train,
  kernel = "radial",  
  gamma = best_gamma,
  cost = best_cost
)

# 使用測試集進行預測
pre_svm_best <- predict(learn_svm_best, heart_test[,-which(names(heart_test) == "output")])

# 計算測試集的混淆矩陣
cm_svm<- confusionMatrix(pre_svm_best, heart_test$output)
print(cm_svm)



```

<!-- #### 深度學習 -->
<!-- ```{R} -->
<!-- # 定義 R² 指標 -->
<!-- r_squared <- function(y_true, y_pred) { -->
<!--   ss_res <- sum((y_true - y_pred) ^ 2) -->
<!--   ss_tot <- sum((y_true - mean(y_true)) ^ 2) -->
<!--   1 - ss_res / ss_tot -->
<!-- } -->

<!-- # 自定義 keras 指標 -->
<!-- r_squared_metric <- custom_metric("r_squared", function(y_true, y_pred) { -->
<!--   r_squared(y_true, y_pred) -->
<!-- }) -->

<!-- # 自定义 RMSE 损失函数 -->
<!-- rmse <- function(y_true, y_pred) { -->
<!--   K <- backend() -->
<!--   return(K$sqrt(K$mean(K$square(y_pred - y_true)))) -->
<!-- } -->


<!-- # 自定义学习率调度函数 -->
<!-- lr_schedule <- function(epoch, lr) { -->
<!--   if (epoch < 10) { -->
<!--     return(lr) -->
<!--   } else if(epoch %% 100 == 0) { -->
<!--     return(lr * exp(-0.1)) -->
<!--   } -->
<!--   else{ -->
<!--     return(lr) -->
<!--   } -->
<!-- } -->

<!-- lr_schedule(1, 0.1) -->
<!-- # 定义回调函数 -->
<!-- lr_callback <- callback_learning_rate_scheduler(schedule = lr_schedule) -->

<!-- ``` -->
<!-- ####  -->

# ```{R }
# 
# 
# 
# x_train <- heart_train[-8]
# y_train <- heart_train[8]
# length(x_train)
# 
# x_train <- as.matrix(x_train)
# y_train <- as.matrix(y_train)
# y_train <- as.numeric(y_train)
# y_train <- as.matrix(y_train)
# 
# ```


# ```{R}
# tf$random$set_seed(222)
# model <- keras_model_sequential()
# 
# model %>%
#   layer_dense(units = 32, input_shape = c(13), activation = 'relu') %>%
#   layer_dropout(rate = 0.2) %>%
#   layer_dense(units = 8, input_shape = c(13), activation = 'relu') %>%
#   layer_dropout(rate = 0.2) %>%
#   layer_dense(units = 4, input_shape = c(13), activation = 'relu') %>%
#   layer_dropout(rate = 0.2) %>%
#   layer_dense(units = 1, activation = 'sigmoid')
# 
# 
# # 編譯模型並設置優化器
# model %>% compile(
#   optimizer = optimizer_adam(),  # 使用 Adam 優化器，學習率為 0.001
#   loss = rmse,       # 損失函數
#   metrics = list(r_squared_metric)                 # 評估指標
# )
# 
# #訓練模型
# history <- model |>
#   fit(x_train, y_train,                # 訓練數據
#   epochs = 800,                     # 訓練的回合數
#   batch_size = 32,                # 批次大小
#   validation_split = 0.20,         # 20% 的數據用於驗證
#   callbacks = list(lr_callback), 
#   verbose = 2
# )
# summary(model)
# 
# print(history)


<!-- ``` -->
<!-- #### 繪圖 -->
<!-- <!-- ```{R} --> -->

<!-- <!-- # 獲取損失值 --> -->
<!-- <!-- loss_values <- history$metrics$loss --> -->
<!-- <!-- val_loss_values <- history$metrics$val_loss --> -->
<!-- <!-- epochs <- 1:length(loss_values) --> -->

<!-- <!-- # 創建數據框 --> -->
<!-- <!-- df <- data.frame( --> -->
<!-- <!--   epoch = epochs, --> -->
<!-- <!--   loss = loss_values, --> -->
<!-- <!--   val_loss = val_loss_values --> -->
<!-- <!-- ) --> -->

<!-- <!-- # 繪製損失值圖表 --> -->
<!-- <!-- p <- ggplot(df, aes(x = epoch)) + --> -->
<!-- <!--   geom_line(aes(y = loss, color = 'Training Loss')) + --> -->
<!-- <!--   geom_line(aes(y = val_loss, color = 'Validation Loss')) + --> -->
<!-- <!--   labs(title = 'Model Loss', --> -->
<!-- <!--        x = 'Epoch', --> -->
<!-- <!--        y = 'Loss') + --> -->
<!-- <!--   scale_color_manual(values = c('Training Loss' = 'blue', 'Validation Loss' = 'red')) + --> -->
<!-- <!--   theme_minimal() --> -->

<!-- <!-- # 顯示圖表 --> -->
<!-- <!-- print(p) --> -->

<!-- <!-- # # 保存圖表為 PNG 文件 --> -->
<!-- <!-- # ggsave("model_loss.png", plot = p) --> -->

<!-- <!-- # 獲取 R² 值 --> -->
<!-- <!-- train_r_squared <- history$metrics$r_squared --> -->
<!-- <!-- val_r_squared <- history$metrics$val_r_squared --> -->
<!-- <!-- epochs <- 1:length(train_r_squared) --> -->

<!-- <!-- # 創建數據框 --> -->
<!-- <!-- df <- data.frame( --> -->
<!-- <!--   epoch = epochs, --> -->
<!-- <!--   train_r_squared = train_r_squared, --> -->
<!-- <!--   val_r_squared = val_r_squared --> -->
<!-- <!-- ) --> -->

<!-- <!-- # 繪製 R² 圖表 --> -->
<!-- <!-- p <- ggplot(df, aes(x = epoch)) + --> -->
<!-- <!--   geom_line(aes(y = train_r_squared, color = 'Training R²')) + --> -->
<!-- <!--   geom_line(aes(y = val_r_squared, color = 'Validation R²')) + --> -->
<!-- <!--   labs(title = 'Model R²', --> -->
<!-- <!--        x = 'Epoch', --> -->
<!-- <!--        y = 'R²') + --> -->
<!-- <!--   scale_color_manual(values = c('Training R²' = 'blue', 'Validation R²' = 'red')) + --> -->
<!-- <!--   theme_minimal() --> -->

<!-- <!-- # 顯示圖表 --> -->
<!-- <!-- print(p) --> -->

<!-- <!-- ``` --> -->




#### 測試集

# ```{r}
# x_test <- heart_test[-8]
# y_test <- heart_test[8]
# x_test <- as.matrix(x_test)
# y_test <- as.matrix(y_test)
# y_test <- as.numeric(y_test)
# y_test <- as.matrix(y_test)
# # 使用測試資料集進行預測
# predictions <- model %>% predict(x_test)
# 
# # 將預測的概率轉換為類別標籤
# predicted_labels <- ifelse(predictions > 0.5, 1, 0)
# 
# # 計算準確率
# accuracy <- mean(predicted_labels == y_test)
# print(paste("測試集準確率:", accuracy))
# 
# # 生成混淆矩陣
# confusionMatrix(factor(predicted_labels), factor(y_test))
# ```







#### 各模型混淆矩陣比較
```{R}
col <-c("#FFD2D2", "#CECEFF")
par(mfrow=c(2,3),cex.main=0.2)
fourfoldplot(cm_lr$table, color = col, conf.level = 0, margin = 1,main=paste("迴歸(",round(cm_lr$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_ct$table, color = col, conf.level = 0, margin = 1, main=paste("決策樹(",round(cm_ct$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_rf$table, color = col, conf.level = 0, margin = 1, main=paste("隨機森林(",round(cm_rf$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_knn$table, color = col, conf.level = 0, margin = 1, main=paste("KNN (",round(cm_knn$overall[1]*100), "%)",sep=""))

fourfoldplot(cm_svm$table, color = col, conf.level = 0, margin = 1, main=paste("SVM(",round(cm_svm$overall[1]*100),"%)", sep=""))



```



#### 特徵選擇(Boruta)
1. 綠色顯示選定的變量
2. 紅色表示拒絕的變量
3. 藍色為判斷基準的變量
4. 黃色（如果有的話）表示 Boruta 尚未決定該變量。

```{R}

default_br <- Boruta(output ~ ., data = heart_all, doTrace = 2, maxRuns = 250)
plot(default_br, las = 2, cex.axis = 2,cex.lab = 2)
# 查看結果摘要
print(default_br)


```

#### 選擇綠色的變量
```{R}
heart_all_br <- heart_all[c(-3,-4,-10,-11)]
heart_train_br <- heart_all_br[inTrain,]
heart_test_br <- heart_all_br[-inTrain,]

```

#### 迴歸模型

```{R}
set.seed(456)
train_control <- trainControl(method = "cv", number = 10)

model_glm <- caret::train(output ~ ., 
               data = heart_train_br, 
               method = "glm", 
               family = binomial,
               trControl = train_control)

print(model_glm)
summary(model_glm)

predictTrain = predict(model_glm, data = heart_train_br, type = "prob")

#print(predictTrain)
table(heart_train_br$output, ifelse(predictTrain[1]>predictTrain[2], 0, 1))

predictTest = predict(model_glm, newdata = heart_test_br, type = "prob")

pre_lr <- ifelse(predictTest[1]>predictTest[2], 0, 1)
pre_lm <- as.factor(pre_lr)
cm_lr_br <- confusionMatrix(pre_lm,heart_test_br$output)
cm_lr_br


```


#### 決策樹
```{r}
library(party)
library(caret)


set.seed(456)

# 制定參數
train_control <- trainControl(
  method = "cv",          # 使用交叉驗證
  number = 10             # 10 折交叉驗證
)

# 定義參數
tune_grid <- expand.grid(
  maxdepth = c(1, 2, 3, 4, 5)  
)

train_model <- function(maxdepth) {
  model <- ctree(
    output ~ .,                
    data = heart_train_br,        
    controls = ctree_control(maxdepth = maxdepth)  # 設置 maxdepth
  )
  # 使用交叉驗證計算模型性能
  pred <- predict(model, heart_train_br[, -6])
  cm <- confusionMatrix(pred, heart_train_br$output)
  return(cm$overall["Accuracy"])
}

# 進行網路搜尋
results <- data.frame(
  maxdepth = integer(),
  Accuracy = numeric()
)

for (depth in tune_grid$maxdepth) {
  accuracy <- train_model(depth)
  results <- rbind(results, data.frame(
    maxdepth = depth,
    Accuracy = accuracy
  ))
}

# 找到最佳參數组合
best_params <- results[which.max(results$Accuracy), ]
print(best_params)

# 使用最佳參數訓練模型
best_model <- ctree(
  output ~ .,                
  data = heart_train_br,        
  controls = ctree_control(maxdepth = best_params$maxdepth)  # 使用最佳的maxdepth
)

# 使用測試集進行預測
pre_df <- predict(best_model, heart_test_br[, -6])

# 計算測試集的混淆矩陣
cm_ct_br <- confusionMatrix(pre_df, heart_test_br$output)
print(cm_ct_br)

# 繪製決策樹
plot(best_model, type = "simple")

```

#### 隨機森林

```{R} 
set.seed(350)
library(randomForest)
library(caret)

# 定義參數
mtry_values <- c(1, 2, 3, 4, 5)
ntree_values <- c(100, 300, 500)
nodesize_values <- c(1, 5, 10)

# 創建空的數據框
results <- data.frame(
  mtry = integer(),
  ntree = integer(),
  nodesize = integer(),
  Accuracy = numeric()
)


for (mtry in mtry_values) {
  for (ntree in ntree_values) {
    for (nodesize in nodesize_values) {
      model <- randomForest(
        output ~ .,               
        data = heart_train_br,        
        ntree = ntree,             
        mtry = mtry,               
        nodesize = nodesize,       
        proximity = TRUE,        
        importance = TRUE          
      )
      
      pre_rf <- predict(model, heart_test_br[,-6])
      cm_rf <- confusionMatrix(pre_rf, heart_test_br$output)
      
      results <- rbind(results, data.frame(
        mtry = mtry,
        ntree = ntree,
        nodesize = nodesize,
        Accuracy = cm_rf$overall['Accuracy']
      ))
    }
  }
}

# 找到最佳参数组合
best_params <- results[which.max(results$Accuracy), ]
print(best_params)


learn_rf <- randomForest(output~.,data = heart_train_br,ntree=best_params$ntree,
             mtry=best_params$mtry,
             nodesize = best_params$nodesize
)
pre_rf <-predict(learn_rf,heart_test_br[,-6])
cm_rf_br <- confusionMatrix(pre_rf,heart_test_br$output)
print(cm_rf_br)
```


### KNN

```{R}
set.seed(456)
train_control <- trainControl(
  method = "cv",          # 使用交叉驗證
  number = 10             # 10 折交叉驗證
)

knn_Fit <- caret::train(
  output~., 
  data = heart_train_br,
  method = "knn",
  trControl = train_control, 
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(k = 1:10),
  metric     = "Accuracy"
)

knn_Fit

#plot(knn_Fit)

knn_Fit.predict <- predict(knn_Fit, newdata = heart_test_br, type = "raw")
table(heart_test_br[, 6], pred=knn_Fit.predict)

cm_knn_br <- confusionMatrix(knn_Fit.predict,heart_test_br$output)
print(cm_knn_br)

```


#### 支援向量機
```{R}

set.seed(456)


# 確保因子水平有效
if (!is.factor(heart_train_br$output)) {
  heart_train_br$output <- as.factor(heart_train_br$output)
}
if (!is.factor(heart_test_br$output)) {
  heart_test_br$output <- as.factor(heart_test_br$output)
}

levels(heart_train_br$output) <- make.names(levels(heart_train_br$output))
levels(heart_test_br$output) <- make.names(levels(heart_test_br$output))

# 定義 SVM 參數網格
tune_grid <- expand.grid(
  .sigma = c(0.01, 0.03, 0.1, 0.3), # gamma 在 e1071 中稱為 sigma
  .C = c(1, 10, 100, 1000) # cost 參數
)

# 訓練 SVM 模型
train_control <- trainControl(
  method = "cv",  # 交叉驗證
  number = 10,    # 10 折交叉驗證
  summaryFunction = twoClassSummary, # 二類問題的評價指標
  classProbs = TRUE  # 計算類別的機率
)

svm_model <- caret::train(
  output ~ ., 
  data = heart_train_br,
  method = "svmRadialSigma",  # 使用svmRadialSigma
  tuneGrid = tune_grid,
  trControl = train_control,
  metric = "Accuracy"  # 使用準確率作為評價標準
)

# 顯示最佳的參數组合
print(svm_model$bestTune)

# 使用最佳的 gamma 和 cost 參數
best_gamma <- svm_model$bestTune$sigma
best_cost <- svm_model$bestTune$C

# 訓練最佳的模型
learn_svm_best <- svm(
  output ~ ., 
  data = heart_train_br,
  kernel = "radial",  
  gamma = best_gamma,
  cost = best_cost
)

# 使用測試集進行預測
pre_svm_best <- predict(learn_svm_best, heart_test_br[,-which(names(heart_test_br) == "output")])

# 計算測試集的混淆矩陣
cm_svm_br<- confusionMatrix(pre_svm_best, heart_test_br$output)
print(cm_svm_br)
```


####

```{R}


```


#### 各模型混淆矩陣比較
```{R}
col <-c("#FFD2D2", "#CECEFF")
par(mfrow=c(2,3),cex.main=0.2)
fourfoldplot(cm_lr_br$table, color = col, conf.level = 0, margin = 1,main=paste("迴歸(",round(cm_lr$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_ct_br$table, color = col, conf.level = 0, margin = 1, main=paste("決策樹(",round(cm_ct$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_rf_br$table, color = col, conf.level = 0, margin = 1, main=paste("隨機森林(",round(cm_rf$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_knn_br$table, color = col, conf.level = 0, margin = 1, main=paste("KNN (",round(cm_knn$overall[1]*100), "%)",sep=""))

fourfoldplot(cm_svm_br$table, color = col, conf.level = 0, margin = 1, main=paste("SVM(",round(cm_svm$overall[1]*100),"%)", sep=""))



```

#```{R echo=FALSE}
# all_png <- image_read('001.png')
# plot(1:2, type='n', xlab='', ylab='', xaxt='n', yaxt='n')
# rasterImage(all_png, 1, 1, 2, 2)
# 
# br_png <- image_read('002.png')
# plot(1:2, type='n', xlab='', ylab='', xaxt='n', yaxt='n')
# rasterImage(br_png, 1, 1, 2, 2)

#```


